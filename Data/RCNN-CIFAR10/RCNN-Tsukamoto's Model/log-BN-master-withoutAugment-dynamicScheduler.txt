RCNN-master
epoch=1 train_loss:2.111171637 train_acc:0.273096955 validating val_loss:1.578209877 val_acc:0.403545673
epoch=2 train_loss:1.559721942 train_acc:0.418609776 validating val_loss:1.328483224 val_acc:0.511818910
epoch=3 train_loss:1.345256278 train_acc:0.511258013 validating val_loss:1.188285232 val_acc:0.572415865
epoch=4 train_loss:1.185705782 train_acc:0.579687500 validating val_loss:1.022411108 val_acc:0.634314904
epoch=5 train_loss:1.073392424 train_acc:0.624358974 validating val_loss:0.943469942 val_acc:0.673978365
epoch=6 train_loss:0.982127930 train_acc:0.657852564 validating val_loss:0.957812190 val_acc:0.663261218
epoch=7 train_loss:0.903404820 train_acc:0.691766827 validating val_loss:0.860923946 val_acc:0.703926282
epoch=8 train_loss:0.826912546 train_acc:0.719571314 validating val_loss:0.775034130 val_acc:0.735677083
epoch=9 train_loss:0.767141313 train_acc:0.742788462 validating val_loss:0.795365512 val_acc:0.733774038
epoch=10 train_loss:0.714354538 train_acc:0.763701923 validating val_loss:0.701021314 val_acc:0.761718750
epoch=11 train_loss:0.668342457 train_acc:0.780208333 validating val_loss:0.701518953 val_acc:0.766826923
epoch=12 train_loss:0.624599158 train_acc:0.795753205 validating val_loss:0.735744536 val_acc:0.754907853
epoch=13 train_loss:0.590104341 train_acc:0.809375000 validating val_loss:0.680785239 val_acc:0.773938301
epoch=14 train_loss:0.553075234 train_acc:0.821053686 validating val_loss:0.623165965 val_acc:0.800981571
epoch=15 train_loss:0.517759218 train_acc:0.834755609 validating val_loss:0.722697377 val_acc:0.776041667
epoch=16 train_loss:0.501985118 train_acc:0.837419872 validating val_loss:0.620776474 val_acc:0.801782853
epoch=17 train_loss:0.472300701 train_acc:0.849278846 validating val_loss:0.710722625 val_acc:0.782451923
epoch=18 train_loss:0.447343719 train_acc:0.856650641 validating val_loss:0.633542717 val_acc:0.805388622
epoch=19 train_loss:0.434843654 train_acc:0.861498397 validating val_loss:0.602899313 val_acc:0.810496795
epoch=20 train_loss:0.405717942 train_acc:0.871935096 validating val_loss:0.637139738 val_acc:0.808994391
epoch=21 train_loss:0.390410253 train_acc:0.874779647 validating val_loss:0.664140284 val_acc:0.805889423
epoch=22 train_loss:0.373324604 train_acc:0.881290064 validating val_loss:0.608483672 val_acc:0.816806891
epoch=23 train_loss:0.363764687 train_acc:0.883473558 validating val_loss:0.661159456 val_acc:0.804987981
epoch=24 train_loss:0.338938728 train_acc:0.891706731 validating val_loss:0.635179579 val_acc:0.808593750
epoch=25 train_loss:0.323851560 train_acc:0.897836538 validating val_loss:0.587317765 val_acc:0.828826122
epoch=26 train_loss:0.314362618 train_acc:0.899659455 validating val_loss:0.628145278 val_acc:0.821013622
epoch=27 train_loss:0.305642778 train_acc:0.901963141 validating val_loss:0.584360719 val_acc:0.827123397
epoch=28 train_loss:0.294019674 train_acc:0.905829327 validating val_loss:0.648006380 val_acc:0.814903846
epoch=29 train_loss:0.285132091 train_acc:0.909455128 validating val_loss:0.594610393 val_acc:0.825320513
epoch=30 train_loss:0.274887544 train_acc:0.912600160 validating val_loss:0.597538710 val_acc:0.823717949
epoch=31 train_loss:0.259590963 train_acc:0.918149038 validating val_loss:0.601069152 val_acc:0.826822917
epoch=32 train_loss:0.251198714 train_acc:0.919951923 validating val_loss:0.638950884 val_acc:0.827724359
epoch=33 train_loss:0.247148156 train_acc:0.920793269 validating val_loss:0.589081824 val_acc:0.828826122
epoch=34 train_loss:0.240972107 train_acc:0.923317308 validating val_loss:0.602979898 val_acc:0.834234776
epoch=35 train_loss:0.238615065 train_acc:0.925180288 validating val_loss:0.637335002 val_acc:0.819911859
epoch=36 train_loss:0.230691395 train_acc:0.925661058 validating val_loss:0.633246362 val_acc:0.826121795
epoch=37 train_loss:0.224341735 train_acc:0.930048077 validating val_loss:0.626216948 val_acc:0.823217147
epoch=38 train_loss:0.213994246 train_acc:0.932612179 validating val_loss:0.613979816 val_acc:0.830328526
Epoch    38: reducing learning rate of group 0 to 1.0000e-02.
epoch=39 train_loss:0.097138154 train_acc:0.969751603 validating val_loss:0.556516826 val_acc:0.855669071
epoch=40 train_loss:0.053125214 train_acc:0.984675481 validating val_loss:0.603284359 val_acc:0.858974359
epoch=41 train_loss:0.037196139 train_acc:0.989322917 validating val_loss:0.662229776 val_acc:0.858673878
epoch=42 train_loss:0.027707971 train_acc:0.992928686 validating val_loss:0.690710008 val_acc:0.859174679
epoch=43 train_loss:0.021318448 train_acc:0.994370994 validating val_loss:0.728260696 val_acc:0.857872596
epoch=44 train_loss:0.017066893 train_acc:0.995392628 validating val_loss:0.771019340 val_acc:0.856971154
epoch=45 train_loss:0.014703066 train_acc:0.996394231 validating val_loss:0.794261992 val_acc:0.859775641
epoch=46 train_loss:0.012969889 train_acc:0.996754808 validating val_loss:0.835913420 val_acc:0.854867788
epoch=47 train_loss:0.010540236 train_acc:0.997395833 validating val_loss:0.851670027 val_acc:0.857471955
epoch=48 train_loss:0.008925680 train_acc:0.997676282 validating val_loss:0.874841928 val_acc:0.856971154
epoch=49 train_loss:0.008352012 train_acc:0.997976763 validating val_loss:0.912663281 val_acc:0.857471955
epoch=50 train_loss:0.006228256 train_acc:0.998417468 validating val_loss:0.926298618 val_acc:0.858373397
Epoch    50: reducing learning rate of group 0 to 1.0000e-03.
epoch=51 train_loss:0.006049896 train_acc:0.998597756 validating val_loss:0.927474380 val_acc:0.859575321
epoch=52 train_loss:0.005288428 train_acc:0.998978365 validating val_loss:0.927237988 val_acc:0.858874199
epoch=53 
train_loss:0.005399601 train_acc:0.998998397 validating val_loss:0.922417104 val_acc:0.857972756
epoch=54 train_loss:0.005188736 train_acc:0.999018429 validating val_loss:0.934525132 val_acc:0.858874199
epoch=55 train_loss:0.004640751 train_acc:0.999198718 validating val_loss:0.912659585 val_acc:0.859575321
epoch=56 train_loss:0.005378192 train_acc:0.998838141 validating val_loss:0.927057505 val_acc:0.858473558
epoch=57 train_loss:0.004946378 train_acc:0.998938301 validating val_loss:0.922523499 val_acc:0.859074519
epoch=58 train_loss:0.004736751 train_acc:0.999118590 validating val_loss:0.930082798 val_acc:0.858273237
epoch=59 train_loss:0.004865491 train_acc:0.998978365 validating val_loss:0.929750085 val_acc:0.859675481
epoch=60 train_loss:0.004605408 train_acc:0.999118590 validating val_loss:0.945662498 val_acc:0.859375000
epoch=61 train_loss:0.004333896 train_acc:0.999379006 validating val_loss:0.934550881 val_acc:0.858774038
Epoch    61: reducing learning rate of group 0 to 1.0000e-04.
epoch=62 train_loss:0.004348601 train_acc:0.999198718 validating val_loss:0.935045779 val_acc:0.858473558
epoch=63 train_loss:0.004268261 train_acc:0.999118590 validating val_loss:0.929330409 val_acc:0.859875801
epoch=64 train_loss:0.004089237 train_acc:0.999258814 validating val_loss:0.946609199 val_acc:0.858673878
epoch=65 train_loss:0.003981916 train_acc:0.999318910 validating val_loss:0.939883888 val_acc:0.859575321
epoch=66 train_loss:0.004135356 train_acc:0.999298878 validating val_loss:0.945621669 val_acc:0.858774038
epoch=67 train_loss:0.004544204 train_acc:0.999138622 validating val_loss:0.943173230 val_acc:0.858874199
epoch=68 train_loss:0.004341060 train_acc:0.999118590 validating val_loss:0.939621985 val_acc:0.859174679
epoch=69 train_loss:0.004476343 train_acc:0.998918269 validating val_loss:0.935546696 val_acc:0.857672276
epoch=70 train_loss:0.005116005 train_acc:0.998898237 validating val_loss:0.934303105 val_acc:0.858273237
