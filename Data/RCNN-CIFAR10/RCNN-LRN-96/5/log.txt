Dropout=0.4
SGD lr=0.01e-03~
start training
validating val_loss:2.305450439 val_acc:0.134815705
epoch=1 train_loss:1.989595054 train_acc:0.240444712 validating val_loss:1.656088233 val_acc:0.398838141
epoch=2 train_loss:1.511202330 train_acc:0.437980769 validating val_loss:1.419854164 val_acc:0.482872596
epoch=3 train_loss:1.290245573 train_acc:0.530228365 validating val_loss:1.192882299 val_acc:0.567708333
epoch=4 train_loss:1.126160783 train_acc:0.596213942 validating val_loss:1.097757697 val_acc:0.602363782
epoch=5 train_loss:0.999539614 train_acc:0.644130609 validating val_loss:0.947942495 val_acc:0.672175481
epoch=6 train_loss:0.906627482 train_acc:0.677043269 validating val_loss:0.867915750 val_acc:0.704627404
epoch=7 train_loss:0.827206426 train_acc:0.709795673 validating val_loss:0.786282599 val_acc:0.726462340
epoch=8 train_loss:0.759423161 train_acc:0.735777244 validating val_loss:0.707853854 val_acc:0.760116186
epoch=9 train_loss:0.697189538 train_acc:0.757211538 validating val_loss:0.673706591 val_acc:0.769731571
epoch=10 train_loss:0.655012679 train_acc:0.772255609 validating val_loss:0.645274699 val_acc:0.782552083
epoch=11 train_loss:0.618416415 train_acc:0.785536859 validating val_loss:0.632085145 val_acc:0.783854167
epoch=12 train_loss:0.581962852 train_acc:0.796053686 validating val_loss:0.591447711 val_acc:0.799879808
epoch=13 train_loss:0.553138793 train_acc:0.807411859 validating val_loss:0.552364528 val_acc:0.807391827
epoch=14 train_loss:0.523956264 train_acc:0.818689904 validating val_loss:0.548732042 val_acc:0.814202724
epoch=15 train_loss:0.499651205 train_acc:0.824659455 validating val_loss:0.530164599 val_acc:0.822215545
epoch=16 train_loss:0.474418408 train_acc:0.833854167 validating val_loss:0.534755588 val_acc:0.816105769
epoch=17 train_loss:0.457705569 train_acc:0.841185897 validating val_loss:0.514865279 val_acc:0.825420673
epoch=18 train_loss:0.434801231 train_acc:0.846794872 validating val_loss:0.512163281 val_acc:0.827223558
epoch=19 train_loss:0.422981952 train_acc:0.852303686 validating val_loss:0.543245375 val_acc:0.817307692
epoch=20 train_loss:0.405372036 train_acc:0.857451923 validating val_loss:0.539962649 val_acc:0.825921474
epoch=21 train_loss:0.395939375 train_acc:0.860176282 validating val_loss:0.487522483 val_acc:0.837439904
epoch=22 train_loss:0.380233383 train_acc:0.866706731 validating val_loss:0.530548751 val_acc:0.827824519
epoch=23 train_loss:0.377830685 train_acc:0.868429487 validating val_loss:0.538647950 val_acc:0.822816506
epoch=24 train_loss:0.361639831 train_acc:0.873257212 validating val_loss:0.503420115 val_acc:0.832832532
epoch=25 train_loss:0.345776325 train_acc:0.877023237 validating val_loss:0.517610550 val_acc:0.835336538
epoch=26 train_loss:0.340479005 train_acc:0.880789263 validating val_loss:0.485822529 val_acc:0.844150641
epoch=27 train_loss:0.326859059 train_acc:0.885256410 validating val_loss:0.467532814 val_acc:0.845252404
epoch=28 train_loss:0.316677717 train_acc:0.889302885 validating val_loss:0.516769826 val_acc:0.830028045
epoch=29 train_loss:0.306586451 train_acc:0.891225962 validating val_loss:0.484013736 val_acc:0.844350962
epoch=30 train_loss:0.304876772 train_acc:0.892688301 validating val_loss:0.489647806 val_acc:0.842548077
epoch=31 train_loss:0.300250187 train_acc:0.891586538 validating val_loss:0.481743068 val_acc:0.847255609
epoch=32 train_loss:0.290224752 train_acc:0.897876603 validating val_loss:0.468727648 val_acc:0.846354167
epoch=33 train_loss:0.279168880 train_acc:0.902063301 validating val_loss:0.507548213 val_acc:0.837640224
epoch=34 train_loss:0.274002870 train_acc:0.902724359 validating val_loss:0.491317123 val_acc:0.846254006
epoch=35 train_loss:0.270723239 train_acc:0.904567308 validating val_loss:0.476763308 val_acc:0.850260417
epoch=36 train_loss:0.259826593 train_acc:0.908573718 validating val_loss:0.490508407 val_acc:0.846153846
epoch=37 train_loss:0.257353649 train_acc:0.909895833 validating val_loss:0.475629508 val_acc:0.853365385
epoch=38 train_loss:0.254943521 train_acc:0.909435096 validating val_loss:0.519967079 val_acc:0.842447917
Epoch    38: reducing learning rate of group 0 to 1.0000e-03.
epoch=39 train_loss:0.170986364 train_acc:0.939783654 validating val_loss:0.448547602 val_acc:0.869290865
epoch=40 train_loss:0.144492029 train_acc:0.949298878 validating val_loss:0.455347270 val_acc:0.871093750
epoch=41 train_loss:0.135341359 train_acc:0.952584135 validating val_loss:0.460488707 val_acc:0.873497596
epoch=42 train_loss:0.129471765 train_acc:0.954366987 validating val_loss:0.469380081 val_acc:0.872495994
epoch=43 train_loss:0.125352975 train_acc:0.954847756 validating val_loss:0.477517068 val_acc:0.873096955
epoch=44 train_loss:0.122842068 train_acc:0.956470353 validating val_loss:0.474563301 val_acc:0.871494391
epoch=45 train_loss:0.117026849 train_acc:0.958253205 validating val_loss:0.484981060 val_acc:0.873096955
epoch=46 train_loss:0.119530756 train_acc:0.957011218 validating val_loss:0.491897941 val_acc:0.870292468
epoch=47 train_loss:0.111621155 train_acc:0.959274840 validating val_loss:0.496512234 val_acc:0.870893429
epoch=48 train_loss:0.110445170 train_acc:0.960076122 validating val_loss:0.500767708 val_acc:0.872896635
epoch=49 train_loss:0.110068257 train_acc:0.959995994 validating val_loss:0.493994057 val_acc:0.873397436
epoch=50 train_loss:0.108394186 train_acc:0.960296474 validating val_loss:0.508488715 val_acc:0.869891827
Epoch    50: reducing learning rate of group 0 to 1.0000e-04.
epoch=51 train_loss:0.100147241 train_acc:0.964383013 validating val_loss:0.501101315 val_acc:0.870993590
epoch=52 train_loss:0.095799120 train_acc:0.965665064 validating val_loss:0.498503625 val_acc:0.871995192
epoch=53 train_loss:0.095355821 train_acc:0.965705128 validating val_loss:0.502116501 val_acc:0.871394231
epoch=54 train_loss:0.099032040 train_acc:0.964222756 validating val_loss:0.502420664 val_acc:0.870793269
epoch=55 train_loss:0.095583443 train_acc:0.965945513 validating val_loss:0.503769457 val_acc:0.872095353
epoch=56 train_loss:0.095620612 train_acc:0.965865385 validating val_loss:0.502123535 val_acc:0.871895032
epoch=57 train_loss:0.094618637 train_acc:0.966286058 validating val_loss:0.505067468 val_acc:0.872696314
epoch=58 train_loss:0.096946066 train_acc:0.965725160 validating val_loss:0.505340576 val_acc:0.870893429
epoch=59 train_loss:0.095702615 train_acc:0.966105769 validating val_loss:0.503280044 val_acc:0.872696314
epoch=60 train_loss:0.093389623 train_acc:0.965985577 validating val_loss:0.504136920 val_acc:0.871895032
epoch=61 train_loss:0.093759308 train_acc:0.966145833 validating val_loss:0.504608572 val_acc:0.873597756
Epoch    61: reducing learning rate of group 0 to 1.0000e-05.
epoch=62 train_loss:0.092176855 train_acc:0.966726763 validating val_loss:0.506482184 val_acc:0.872896635
epoch=63 train_loss:0.093301175 train_acc:0.966386218 validating val_loss:0.506164372 val_acc:0.872596154
epoch=64 train_loss:0.092465590 train_acc:0.966125801 validating val_loss:0.505425513 val_acc:0.873096955
epoch=65 train_loss:0.093194613 train_acc:0.966666667 validating val_loss:0.505781054 val_acc:0.872696314
epoch=66 train_loss:0.090872815 train_acc:0.967788462 validating val_loss:0.505301654 val_acc:0.873197115
epoch=67 train_loss:0.095783212 train_acc:0.965244391 validating val_loss:0.506080925 val_acc:0.872696314
epoch=68 train_loss:0.092483185 train_acc:0.966907051 validating val_loss:0.506716430 val_acc:0.872295673
epoch=69 train_loss:0.092879673 train_acc:0.966326122 validating val_loss:0.506065607 val_acc:0.872896635
epoch=70 train_loss:0.091244480 train_acc:0.967447917 validating val_loss:0.506866097 val_acc:0.872495994
epoch=71 train_loss:0.092033600 train_acc:0.967207532 validating val_loss:0.506862402 val_acc:0.872696314
